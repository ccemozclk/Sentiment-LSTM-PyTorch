{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "L4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction & Setup :"
   ],
   "metadata": {
    "id": "a0f0d8PYbe8n"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rYHM-vLbWGH"
   },
   "outputs": [],
   "source": [
    "!pip install datasets kagglehub -q\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"The model will work on this device.: {device}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRjsVIeGbjyD",
    "outputId": "31862528-a340-4c5f-ffac-0ee68eec0892"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions to be used for data manipulation :"
   ],
   "metadata": {
    "id": "T30ov2G3cDPX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_and_tokenize(text):\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    return text.lower().split()"
   ],
   "metadata": {
    "id": "CsREw9o8cWBn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def build_vocab_smart(input_data, max_vocab_size=20000):\n",
    "    word_counts = Counter()\n",
    "\n",
    "    for item in input_data:\n",
    "\n",
    "        if isinstance(item, list):\n",
    "            tokens = item\n",
    "\n",
    "        elif isinstance(item, dict) and 'text' in item:\n",
    "            tokens = preprocess_and_tokenize(item['text'])\n",
    "\n",
    "        elif isinstance(item, str):\n",
    "            tokens = preprocess_and_tokenize(item)\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        word_counts.update(tokens)\n",
    "\n",
    "    most_common_words = word_counts.most_common(max_vocab_size)\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "\n",
    "    for word, count in most_common_words:\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "    print(f\"✅ Vocab Ready! Size: {len(vocab)}\")\n",
    "    return vocab"
   ],
   "metadata": {
    "id": "jHjUg6sUcZJ9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def encode_and_pad(input_data, vocab, max_len=250):\n",
    "    if isinstance(input_data, str):\n",
    "        input_data = [input_data]\n",
    "\n",
    "    encoded_batch = []\n",
    "    pad_id = vocab.get('<PAD>', 0)\n",
    "    unk_id = vocab.get('<UNK>', 1)\n",
    "\n",
    "    for item in input_data:\n",
    "        if isinstance(item, list):\n",
    "            tokens = item\n",
    "        else:\n",
    "            tokens = preprocess_and_tokenize(item)\n",
    "\n",
    "        encoded = [vocab.get(word, unk_id) for word in tokens]\n",
    "\n",
    "        if len(encoded) < max_len:\n",
    "            padding = [pad_id] * (max_len - len(encoded))\n",
    "            encoded = padding + encoded\n",
    "        else:\n",
    "            encoded = encoded[:max_len]\n",
    "\n",
    "        encoded_batch.append(encoded)\n",
    "\n",
    "    if len(encoded_batch) == 1 and len(input_data) == 1:\n",
    "         return encoded_batch[0]\n",
    "\n",
    "    return encoded_batch"
   ],
   "metadata": {
    "id": "hBljvB54i4sC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds == y).float()\n",
    "    return correct.sum() / len(correct)"
   ],
   "metadata": {
    "id": "zy-SuKJacteQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_sentiment(text, model, vocab, device):\n",
    "    model.eval()\n",
    "\n",
    "    encoded = encode_and_pad(text, vocab)\n",
    "\n",
    "    tensor_text = torch.tensor(encoded).unsqueeze(0).to(device)\n",
    "\n",
    "    h = None\n",
    "    with torch.no_grad():\n",
    "        prediction, _ = model(tensor_text, h)\n",
    "\n",
    "    score = prediction.item()\n",
    "    status = \"Positive\" if score > 0.5 else \"Negative\"\n",
    "\n",
    "    print(f\"Input: '{text}'\")\n",
    "    print(f\"Score: {score:.4f} -> Result: {status}\")\n",
    "    print(\"-\" * 30)\n"
   ],
   "metadata": {
    "id": "wiKUYlajcwoh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # Concatenate the final forward and backward hidden states\n",
    "        hidden_last_layer = torch.cat((hidden[0][-2,:,:], hidden[0][-1,:,:]), dim=1)\n",
    "\n",
    "        out = self.dropout(hidden_last_layer)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        # For bidirectional LSTM, num_layers * 2\n",
    "        number_of_layers = self.n_layers * 2\n",
    "\n",
    "        hidden = (weight.new(number_of_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(number_of_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden"
   ],
   "metadata": {
    "id": "Fabnq9BikNGO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_history(train_loss, val_loss):\n",
    "    steps = range(100, len(train_loss) * 100 + 1, 100)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.plot(steps, train_loss, 'b-', label='Training Loss')\n",
    "    plt.plot(steps, val_loss, 'r-', label='Validation Loss')\n",
    "\n",
    "    plt.title('Training & Validation Loss (Step-by-Step)')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "m7clu2UmVhmd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Phase 1: The Kaggle Dataset :"
   ],
   "metadata": {
    "id": "gbJXPZbrbzgb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading Process :"
   ],
   "metadata": {
    "id": "2MZg12Knb9KM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Phase 1: Downloading Kaggle Dataset...\")\n",
    "\n",
    "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "\n",
    "print(f\"Data Set Path: {path}\")\n",
    "\n",
    "csv_path = os.path.join(path, \"IMDB Dataset.csv\")\n",
    "\n",
    "try:\n",
    "    df_kaggle = pd.read_csv(csv_path)\n",
    "    print(\"\\n--- Kaggle Dataset (First 5 Rows) ---\")\n",
    "    print(df_kaggle.head())\n",
    "    print(f\"Dataset Shape: {df_kaggle.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: CSV file not found in the specified path.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSbX1iKqbqqv",
    "outputId": "9b0cebd3-5a3d-4f2e-db72-fd2c7a7fcafd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Data is being cleaned...\")\n",
    "# Using the updated preprocess_and_tokenize function\n",
    "cleaned_reviews = [preprocess_and_tokenize(review) for review in df_kaggle['review']]\n",
    "print(\"Data has been cleaned\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-A5v-JyooYJ",
    "outputId": "8de939b4-7330-4d4b-a24d-79e67cb539ca"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "labels = [1 if sentiment == \"positive\" else 0 for sentiment in df_kaggle['sentiment']]\n",
    "labels = np.array(labels)"
   ],
   "metadata": {
    "id": "d24HC1qzo3lp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Use build_vocab_smart instead of build_vocab_optimized\n",
    "vocab_kaggle = build_vocab_smart(cleaned_reviews, max_vocab_size=20000)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lc2lcRi_o8s4",
    "outputId": "39ae4653-9d78-4ada-cb9a-a291c9bdf412"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"The ID of the word 'film': {vocab_kaggle.get('film')}\")\n",
    "print(f\"The ID of a non-existent word (If it returns None, we will set it to 1 in the encode part): {vocab_kaggle.get('zartzurt')}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QxmZxP5o_mO",
    "outputId": "2e37b45d-47a3-445c-96d6-d0c6acb6bbe4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "encoded_list = encode_and_pad(cleaned_reviews, vocab_kaggle, max_len=250)\n",
    "input_tensor_kg = torch.tensor(encoded_list)\n",
    "target_tensor_kg = torch.tensor(labels).float()\n",
    "\n",
    "if isinstance(labels, torch.Tensor):\n",
    "    target_tensor = labels.clone().detach().float()\n",
    "else:\n",
    "    target_tensor = torch.tensor(labels).float()\n",
    "\n",
    "if len(input_tensor_kg) != len(target_tensor_kg):\n",
    "    print(f\"⚠️ WARNING: Size mismatch detected! (Input: {len(input_tensor_kg)}, Target: {len(target_tensor_kg)})\")\n",
    "    print(\"⚠️ Truncating/Aligning Target tensor to Input size to continue processing.\")\n",
    "\n",
    "\n",
    "    if len(input_tensor_kg) < len(target_tensor_kg):\n",
    "        target_tensor_kg = target_tensor[:len(input_tensor_kg)]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Target (Labels) data is LESS than Input data! Please check 'labels' variable.\")\n",
    "\n",
    "split_idx_1 = int(len(input_tensor_kg) * 0.8)\n",
    "split_idx_2 = int(len(input_tensor_kg) * 0.9)\n",
    "\n",
    "train_x, val_x, test_x = input_tensor_kg[:split_idx_1], input_tensor_kg[split_idx_1:split_idx_2], input_tensor_kg[split_idx_2:]\n",
    "train_y, val_y, test_y = target_tensor_kg[:split_idx_1], target_tensor_kg[split_idx_1:split_idx_2], target_tensor_kg[split_idx_2:]\n",
    "\n",
    "print(f\"Train Set: {train_x.shape}\")\n",
    "print(f\"Validation Set: {val_x.shape}\")\n",
    "print(f\"Test Set: {test_x.shape}\")\n",
    "\n",
    "batch_size_kaggle = 64\n",
    "\n",
    "train_data = TensorDataset(train_x, train_y)\n",
    "train_loader_kaggle = DataLoader(train_data, shuffle=True, batch_size=batch_size_kaggle)\n",
    "\n",
    "val_data = TensorDataset(val_x, val_y)\n",
    "val_loader_kaggle = DataLoader(val_data, shuffle=True, batch_size=batch_size_kaggle)\n",
    "\n",
    "test_data = TensorDataset(test_x, test_y)\n",
    "test_loader_kaggle = DataLoader(test_data, shuffle=True, batch_size=batch_size_kaggle)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d51_acCdpD9X",
    "outputId": "884c2f88-9cc8-42d1-f390-4521a56843b8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vocab_size_k = len(vocab_kaggle) + 1\n",
    "embedding_dim_k = 400\n",
    "hidden_dim_k = 256\n",
    "output_dim_k = 1\n",
    "n_layers_k = 2\n",
    "drop_prob = 0.5\n",
    "lr_k = 0.001\n",
    "epochs_k = 5\n",
    "print_every_k = 100\n",
    "clip_k = 5\n",
    "counter = 0\n",
    "batch_size_kaggle = 64\n",
    "\n",
    "model_kaggle = SentimentLSTM(vocab_size_k, embedding_dim_k, hidden_dim_k, output_dim_k, n_layers_k,drop_prob)\n",
    "model_kaggle.to(device)\n",
    "print(model_kaggle)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RkliW42dpb3r",
    "outputId": "61219b41-87a1-445d-d55b-d902b095e8c9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_kaggle.parameters(), lr=lr_k)\n",
    "\n",
    "model_kaggle.train()\n",
    "\n",
    "train_losses_history_kaggle = []\n",
    "val_losses_history_kaggle = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs_k):\n",
    "    h = model_kaggle.init_hidden(batch_size_kaggle)\n",
    "    model_kaggle.train()\n",
    "    train_losses = []\n",
    "    counter = 0\n",
    "\n",
    "    for inputs, labels in train_loader_kaggle:\n",
    "        counter += 1\n",
    "        if inputs.size(0) != batch_size_kaggle: continue\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        model_kaggle.zero_grad()\n",
    "        output, h = model_kaggle(inputs, h)\n",
    "\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model_kaggle.parameters(), clip_k)\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if counter % print_every_k == 0:\n",
    "            val_h = model_kaggle.init_hidden(batch_size_kaggle)\n",
    "            val_losses = []\n",
    "            val_accs = []\n",
    "            model_kaggle.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader_kaggle:\n",
    "                    if inputs.size(0) != batch_size_kaggle: continue\n",
    "\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "                    output, val_h = model_kaggle(inputs, val_h)\n",
    "\n",
    "                    val_loss = criterion(output.squeeze(), labels)\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                    preds = torch.round(output.squeeze())\n",
    "                    acc = (preds == labels).float().mean()\n",
    "                    val_accs.append(acc.item())\n",
    "\n",
    "            model_kaggle.train()\n",
    "\n",
    "            this_val_loss = np.mean(val_losses)\n",
    "            this_val_acc = np.mean(val_accs)\n",
    "\n",
    "            train_losses_history_kaggle.append(loss.item())\n",
    "            val_losses_history_kaggle.append(this_val_loss)\n",
    "\n",
    "            print(f\"Epoch: {epoch+1}/{epochs_k}...\",\n",
    "                  f\"Step: {counter}...\",\n",
    "                  f\"Loss: {loss.item():.6f}...\",\n",
    "                  f\"Val Loss: {this_val_loss:.6f}\",\n",
    "                  f\"Val Acc: %{this_val_acc*100:.2f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TZtvPvnarFLY",
    "outputId": "924e6a7c-70d5-4f3f-95cb-53eb841e3b45"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_history(train_losses_history_kaggle, val_losses_history_kaggle)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "TmvTiMKPrieM",
    "outputId": "3fac30c5-ecfd-4389-cb37-715d09061182"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "- As can be seen from the model outputs and graph above, our model undergoes overfitting after a certain point (see Step 2200 - Epoch: 4). The graph clearly shows that the Training_Loss and Validation_Loss values ​​begin to diverge between Steps 2200 and 2400. Our model achieved its best val_Loss at Step 2200, reaching a val_loss of 0.312; our val_acc value was 87.58%.\n",
    "\n",
    "- These scores, obtained using only LSTM with a synthetic dataset from Kaggle and without any BERT or Transformer architecture, are acceptable results. Our LSTM model understands words, can decipher sentence structures and irony, and its generalization ability is at the desired level. I want to test our model against the best-performing point we have. Let's see the result together."
   ],
   "metadata": {
    "id": "zXhhXv6WXhm7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_final = SentimentLSTM(vocab_size_k, embedding_dim_k, hidden_dim_k, output_dim_k, n_layers_k, drop_prob)\n",
    "model_final.to(device)\n",
    "optimizer = torch.optim.Adam(model_final.parameters(), lr=lr_k)\n",
    "\n",
    "for epoch in range(3):\n",
    "    h = model_final.init_hidden(batch_size_kaggle)\n",
    "    model_final.train()\n",
    "\n",
    "    for inputs, labels in train_loader_kaggle:\n",
    "        if inputs.size(0) != batch_size_kaggle: continue\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        model_final.zero_grad()\n",
    "        output, h = model_final(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model_final.parameters(), clip_k)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "torch.save(model_final.state_dict(), 'sentiment_bilstm_final.pt')"
   ],
   "metadata": {
    "id": "97vRO2koVou1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "challenge_sentences = [\n",
    "    \"I hated this film, it was a total waste of time.\",\n",
    "    \"This movie was absolutely amazing and the acting was great.\",\n",
    "    \"I really wanted to like this movie but I couldn't.\",\n",
    "    \"The cinematography was good but the plot was boring.\",\n",
    "    \"It was not bad at all.\",\n",
    "    \"Best movie ever? I don't think so.\"\n",
    "]\n",
    "\n",
    "\n",
    "for sent in challenge_sentences:\n",
    "    predict_sentiment(sent, model_final, vocab_kaggle, device)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bC1UWnGYiOk",
    "outputId": "259d394c-e965-4c29-dcdf-b7ab9a08a7be"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**The model perfectly solved the \"routine\" sentences (I hated it, I loved it, but...). Even correctly identifying a complex structure like \"I really wanted to like...\" proves that LSTM works.**\n",
    "\n",
    "- Why Did It Go Wrong? (The Failure Analysis)**\n",
    "\n",
    "    * 1.\"Best movie ever? I don't think so.\" -> (Why did it say Positive?)\n",
    "\n",
    "      - **Criminal 1:** Tokenizer. The re.findall(r\"\\w+\", text) function we used removes punctuation marks.\n",
    "\n",
    "      - What the model sees is: best movie ever i don't think so\n",
    "\n",
    "      - Because the question mark (?) is gone, the sarcastic/questioning tone is lost.\n",
    "\n",
    "      - **Criminal 2:** Word Weights. The words \"best,\" \"movie,\" and \"ever\" are so positive in the Embedding space that the faint \"don't think so\" at the end wasn't enough to erase this massive positive wave (triggering LSTM's Forget Gate).\n",
    "\n",
    "    * 2.\"It was not bad at all.\" -> (Why did it say negative?)\n",
    "\n",
    "      - **The criminal:** The dominance of the word \"bad.\"\n",
    "\n",
    "      - In the training set (IMDB), the word \"bad\" appears in negative reviews 95% of the time. The structure \"not bad\" is less common.\n",
    "\n",
    "      - When LSTM reads sequentially, it sees \"not,\" then when it sees \"bad,\" it thinks, \"Okay, this is definitely something bad.\" The phrase \"at all\" seems neutral to it and it can't nullify the effect of \"bad.\""
   ],
   "metadata": {
    "id": "-y86v5MhbUFi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Phase 2 : HugginFace IMDB Dataset :"
   ],
   "metadata": {
    "id": "GhWjtm6IiHtF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "try:\n",
    "  dataset = load_dataset(\"imdb\")\n",
    "  print(\"Success! Dataset loaded.\")\n",
    "  print(f\"Training Data Size: {len(dataset['train'])} comments\")\n",
    "  print(f\"Test Data Size: {len(dataset['test'])} comments\")\n",
    "\n",
    "  print(\"\\n--- Example Data (Raw) ---\")\n",
    "  print(dataset['train'][0])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An Error Occured: {e}\")"
   ],
   "metadata": {
    "id": "wVG2m7T0Y-LD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e20739da-15fa-4692-d2b1-8f13b2fc608a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "word_counts = Counter()\n",
    "\n",
    "for example in dataset['train']:\n",
    "    # Use preprocess_and_tokenize here\n",
    "    tokens = preprocess_and_tokenize(example['text'])\n",
    "    word_counts.update(tokens)\n",
    "\n",
    "vocab_hf = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "sorted_words = word_counts.most_common(20000)\n",
    "\n",
    "for word, count in sorted_words:\n",
    "    vocab_hf[word] = len(vocab_hf)"
   ],
   "metadata": {
    "id": "sedhPV55mxAP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_x_list = [encode_and_pad(text, vocab_hf) for text in dataset['train']['text']]\n",
    "train_x = torch.tensor(train_x_list)\n",
    "train_y = torch.tensor([label for label in dataset['train']['label']]).float()\n",
    "\n",
    "test_x_list = [encode_and_pad(text, vocab_hf) for text in dataset['test']['text']]\n",
    "test_x = torch.tensor(test_x_list)\n",
    "test_y = torch.tensor([label for label in dataset['test']['label']]).float()\n",
    "\n",
    "\n",
    "print(f\"✅ Train Shape: {train_x.shape}\")\n",
    "print(f\"✅ Test Shape: {test_x.shape}\")\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(train_x, train_y), shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(test_x, test_y), shuffle=False, batch_size=batch_size)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWHjE9Vik87I",
    "outputId": "6c810b29-bb24-4825-c10b-4faddd14fb51"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vocab_size_hf = len(vocab_hf) + 1\n",
    "embedding_dim_hf = 128\n",
    "hidden_dim_hf = 256\n",
    "output_dim_hf = 1\n",
    "n_layers_hf = 2\n",
    "batch_size_hf = 64\n",
    "print_every_hf = 100\n",
    "epochs_hf = 5\n",
    "clip_hf = 5\n",
    "drop_prob = 0.5\n",
    "lr_hf = 0.001\n",
    "\n",
    "model_hf = SentimentLSTM(vocab_size_hf, embedding_dim_hf, hidden_dim_hf, output_dim_hf, n_layers_hf,drop_prob)\n",
    "model_hf.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_hf.parameters(), lr=lr_hf)\n",
    "\n",
    "model_hf.train()\n",
    "counter = 0\n",
    "\n",
    "train_losses_history_hf = []\n",
    "val_losses_history_hf = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs_hf):\n",
    "    h = model_hf.init_hidden(batch_size_hf)\n",
    "    model_hf.train()\n",
    "    train_losses = []\n",
    "    counter = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        if inputs.size(0) != batch_size_hf: continue\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        model_hf.zero_grad()\n",
    "        output, h = model_hf(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model_hf.parameters(), clip_hf)\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if counter % print_every_hf == 0:\n",
    "            val_h = model_hf.init_hidden(batch_size_hf)\n",
    "            val_losses = []\n",
    "            val_accs = []\n",
    "            model_hf.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    if inputs.size(0) != batch_size_hf: continue\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "                    output, val_h = model_hf(inputs, val_h)\n",
    "                    val_loss = criterion(output.squeeze(), labels)\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                    preds = torch.round(output.squeeze())\n",
    "                    acc = (preds == labels).float().mean()\n",
    "                    val_accs.append(acc.item())\n",
    "\n",
    "            model_hf.train()\n",
    "\n",
    "            this_val_loss = np.mean(val_losses)\n",
    "            this_val_acc = np.mean(val_accs)\n",
    "\n",
    "            train_losses_history_hf.append(loss.item())\n",
    "            val_losses_history_hf.append(this_val_loss)\n",
    "\n",
    "            print(f\"Epoch: {epoch+1}/{epochs_hf} | Step: {counter} | \"\n",
    "                  f\"Loss: {loss.item():.4f} | Val Loss: {this_val_loss:.4f} | \"\n",
    "                  f\"Val Acc: %{this_val_acc*100:.2f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-3iu9rn4nru4",
    "outputId": "2f003da8-8d25-476c-b355-9ab329214368"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_history(train_losses_history_hf, val_losses_history_hf)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "hhcBMyV7rlOj",
    "outputId": "54fc36fd-9638-4652-cc06-13866b6fc578"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_final_hf = SentimentLSTM(vocab_size_hf, embedding_dim_hf, hidden_dim_hf, output_dim_hf, n_layers_hf, drop_prob)\n",
    "model_final_hf.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_final_hf.parameters(), lr=lr_hf)\n",
    "\n",
    "for epoch in range(5):\n",
    "    h = model_final_hf.init_hidden(batch_size_hf)\n",
    "    model_final_hf.train()\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "\n",
    "        if inputs.size(0) != batch_size_hf: continue\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        model_final_hf.zero_grad()\n",
    "        output, h = model_final_hf(inputs, h)\n",
    "\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model_final_hf.parameters(), clip_hf)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/5 Has Completed !\")\n",
    "\n",
    "\n",
    "torch.save(model_final_hf.state_dict(), 'sentiment_bilstm_hf_final.pt')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzyC-zAEyskg",
    "outputId": "c470c199-6bc4-4d85-e6b9-4484d425b303"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "challenge_sentences = [\n",
    "    \"I hated this film, it was a total waste of time.\",\n",
    "    \"This movie was absolutely amazing and the acting was great.\",\n",
    "    \"I really wanted to like this movie but I couldn't.\",\n",
    "    \"The cinematography was good but the plot was boring.\",\n",
    "    \"It was not bad at all.\",\n",
    "    \"Best movie ever? I don't think so.\"\n",
    "]\n",
    "\n",
    "for sent in challenge_sentences:\n",
    "\n",
    "    predict_sentiment(sent, model_final_hf, vocab_hf, device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8tbcFe9Q9Co",
    "outputId": "60d5943a-3688-427e-a990-0157cb9460c2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Ytxjk4o0USpI"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}